# AVL trees

balenced BST

balence factor = height(Tr) - height(Tl) 

insertion:

​	max 1 rotation

deletion:

​	more than 1 rotation



##   Note for deletion in AVL

remember to check _ensure_balance when you rewind the recursion

for two child removal, swap node with its IOP (should be right most leaf on its left-subtree). Note in this swap, we don't change the pointer, we only change the content in two node

```c++
template <class K, class V>
void AVLTree<K, V>::swap(Node*& first, Node*& second)
{
    K tempkey = first->key;
    V tempvalue = first->value;
    first->key = second->key;
    first->value = second->value;
    second->key = tempkey;
    second->value = tempvalue;
}
```

After swaping , call remove(node->left, key) again to remove it. In this way, we will go down from current node to the location of IOP, so that we can make sure the balance of this path when we rewind the recursion.



# Btree

order of m => max of m children + max of (m-1) <key,value>

 A B-trees of order m is an m-way tree:

1. All keys within a node are ordered (findIndex)

2. All internal nodes have exactly one more children than keys

3. All leaves are on the same level (that's why it is a self-balancing tree)

4. Root node have [1,m-1] keys

5. Non-root internal nodes have [ceil(m/2)-1,m-1] keys

      root is generated by splitting low-level internal nodes. And internal nodes will not be splitted until it reaches full capacity



## Insertion

  Find the leaf node (recursively) and insert in the leaf node (keeping sorted property)

  If the leaf has m-1 nodes, we must split the node by throwing the middle element to its parent

  We will check if split is needed when we rewind the recursion after insertion into the leaf. Note that we don't check the subnode itself, we check its child to be inserted instead. If the child > max, we split the child and throw up the middle one into current subnode. 



# Hash table

A good hash function:

 1. computation time at O(1)

 2. Deterministic

 3. SUHA - simple uniform hashing assumption

      for any key k1 != k2, p(h(k1)==h(k2)) = 1/N (uniform distribution) 

      -- Evenly distribute items in the hash table

       -- Each item has equal probability to be placed into a slot



## Open hashing  -- seperate chaining

keep data outside the hash table 

Each table holds a pointer to the head of a linked list 

  useful if data is large (insert into a linked list at front is easy and no copy is needed). Not good in remove/find (O(alpha) under SUHA)

## Closed hashing 

keep data inside the hash table

  useful when we don't care about the size 

### Linear probing 

i++ until we find an empty spot (use modulus function to wrap around and continue searching from the start of table)

i = (i++)%size;

drawback is the primary clusters. Items have the tendency to be hashed into a cluster. And the bigger this cluster it gets, the faster it grows.

### Double hasing 

Idea is to use a step function to avoid the primary cluster

h = (h1 + i*h2) % size;



### Load factor

  number of elems / the size of table 

require this < 0.66 (2/3)

So we need to resize the table to make load factor small -- doubling the size and re-hashing. Make sure the new size is prime to avoid infinit loop



# Heap

A normal queue: first in, first out

A priority queue: highest priority, first out

##   ADT

is_empty

insert

pop: remove the highest priority and maintain the heap property.

peek - return the hightest priority element but does not modify the queue = O(1)

we just return array[root()]. The root() is either 0 or 1 and access elem in array is only O(1)



##   (min) Heap

A complete binary tree. We use the tree only as the visualization and we use the array as implementations.

###     Insert

​		double the size if full

​		insert at end

​		heapifyup (keep comparing with parent and swap if needed)

​		runtime = o(h) = o(logn) complete tree



###     RemoveMin()

​		sawp the last element with the root

​	    return last element

​		heapifyDown(newroot) 

​		runtime = o(h) = o(logn) complete tree



### 		BuildHeap()

   1. sort O(nlogn) and fill in 

   2. call insert n times O(nlogn)

   3. for(i=parent[size-1]; i>0; --i) heapifyDown

      Since each leaf is a heap by itself so we start from 1st non leaf, which is the parent of the last elem.	





### Heap sort

time complexity = O(nlogn)

  = build heap (O(n)) + pop from heap ( nlog(n)) : each removal takes logn since we have to heapifydown.

And the way we pop the root is to replae the root (index=1) with the last element, heapify down the new root and reduce the heap size by 1. So for a min heap, after removing all elements from the heap, the new array is in descending order. So if we are looking/searching for largest elements, we use minHeap.

### Heap search

time complexity = O(nlogk)

We use minHeap: we only wanna keep kth largest elements in the heap so we have to eject smaller items. That's why we use minHeap. So we first insert items into heap until the size reaches k. And we compare the new item with the root(peek() only takes O(1) time), if it's smaller than root, we don't even bother to insert this one. If it's larger than root, we insert at O(logK). We use K here because there are only k elements in the heap.

